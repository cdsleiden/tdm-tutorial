
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>11. Differences and similarities &#8212; Text and Data Mining Tutorial</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="12. Topic modelling" href="12%20Topic%20modelling.html" />
    <link rel="prev" title="10. Named Entity Recognition" href="10%20Named_Entity_Recognition.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Text and Data Mining Tutorial</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Welcome.html">
   Welcome
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorial
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1%20Acquiring%20texts.html">
   1. Acquiring texts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2%20Tokenisation.html">
   2. Word frequencies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3%20Word%20frequencies.html">
   3. Word frequencies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4%20Examining_context.html">
   4. Examining the context of words
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5%20Type-token_ratio.html">
   5. Type-token ratio
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6%20NLP.html">
   6. NLP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="7%20Sentiment_analysis.html">
   7. Sentiment analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="8%20Lexicons.html">
   8. Lexicons
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="9%20Diction.html">
   9. Diction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10%20Named_Entity_Recognition.html">
   10. Named Entity Recognition
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   11. Differences and similarities
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12%20Topic%20modelling.html">
   12. Topic modelling
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Solutions to the exercises
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Solutions/1%20Acquiring%20texts.html">
   1. Acquiring texts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Solutions/2%20Tokenisation.html">
   2. Tokenisation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Solutions/3%20Word%20frequencies.html">
   3. Word frequencies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Solutions/4%20Examining_context.html">
   4. Examining context
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Solutions/5%20Type-token_ratio.html">
   5. Type-token ratio
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Solutions/6%20NLP.html">
   6. NLP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Solutions/7%20Sentiment_analysis.html">
   7. Sentiment analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Solutions/8%20Lexicons.html">
   8. Lexicons
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Solutions/9%20Diction.html">
   9. Diction
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/11 Differences_and_Similarities.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/cdsleiden/tdm-tutorial/gh-pages?urlpath=tree/docs/notebooks/11 Differences_and_Similarities.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#euclidean-distance">
   Euclidean distance
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cosine-similarity-and-cosine-distance">
   Cosine similarity and cosine distance
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scikit-learn">
   Scikit-learn
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-term-document-matrix">
   A term-document matrix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-heat-map">
   A heat map
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-dendrogram">
   A dendrogram
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-normalised-term-document-matrix">
   A normalised term-document matrix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-network">
   A network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#principal-component-analysis">
   Principal component analysis
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="differences-and-similarities">
<h1>11. Differences and similarities<a class="headerlink" href="#differences-and-similarities" title="Permalink to this headline">¶</a></h1>
<p>Researchers in the humanities and the social sciences are often interested in the differences and similarities between texts. Studies may concentrate on the differences between texts in separate genres, or on the similarities between texts from distinct historical periods. Analyses such as these may be based on a wide range of textual aspects, such as the frequencies of the words that are used in texts, the average lengths of the sentences, or the proportions of nouns and adjectives. Using the values generated for metrics such as these, scholars can try to divide a collection of texts into smaller groups. The texts which display low values for a specific set of variables may all be placed in the same group, for example. It may then be interesting to examine whether the groups that can be created using such statistical analyses are similar in one way or another to other types of divisions, such as those based genre, year of creation or theme.</p>
<div class="section" id="euclidean-distance">
<h2>Euclidean distance<a class="headerlink" href="#euclidean-distance" title="Permalink to this headline">¶</a></h2>
<p>The differences between two collections of numbers can firstly be examined by calculating the euclidean distance.</p>
<p>Suppose that you have two different texts. The first of these mention the word ‘sun’ 10 times and the the word ‘moon’ 26 times. In the second text, the freqencies of these same words are 17 and 10. These numbers can be visualised in a scatter plot using the code below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">26</span><span class="p">]</span> <span class="p">,</span>  <span class="p">[</span><span class="mi">17</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span> <span class="p">]</span> 

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;sun&#39;</span><span class="p">,</span> <span class="s1">&#39;moon&#39;</span> <span class="p">]</span> <span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="p">[</span> <span class="s1">&#39;Text1&#39;</span><span class="p">,</span><span class="s1">&#39;Text2&#39;</span> <span class="p">]</span> <span class="p">)</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span> <span class="n">data</span> <span class="o">=</span> <span class="n">df</span> <span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;sun&#39;</span> <span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;moon&#39;</span> <span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">20</span> <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">35</span><span class="p">)</span>

<span class="k">for</span> <span class="n">index</span><span class="p">,</span><span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;sun&#39;</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;moon&#39;</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span> <span class="p">)</span>

    
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="p">[</span><span class="mi">17</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">26</span><span class="p">],</span> <span class="s1">&#39;r:&#39;</span> <span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="p">[</span><span class="mi">17</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">],</span> <span class="s1">&#39;b:&#39;</span> <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">26</span><span class="p">],</span> <span class="s1">&#39;b:&#39;</span> <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/11 Differences_and_Similarities_2_0.png" src="../_images/11 Differences_and_Similarities_2_0.png" />
</div>
</div>
<p>The euclidean distance between these two points is the length of the line connecting these points. In the scattor plot above, this is the red dotted line. The length of this line can be calculated using the Pythagorean theorem. Given the lengths of the two blue lines, 7 and 16, it can be calculated that the length of the blue line is 17,64.</p>
<p>In the example that was given, the distance between the two texts was calculated using two variables only. It is also possible, however, to expand the number of variables. With more than two variables, we need to calculate the length of a vector representing distances in a multi-dimensional space.</p>
</div>
<div class="section" id="cosine-similarity-and-cosine-distance">
<h2>Cosine similarity and cosine distance<a class="headerlink" href="#cosine-similarity-and-cosine-distance" title="Permalink to this headline">¶</a></h2>
<p>Alternatively, you can also compere texts by calculating the <em>cosine similarity</em>. The cosine similarity between two points is calculated by firstly connecting both of these points to the origin of the graph (i.e. the point where both <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> are 0). You can see an illustration of this in the scatter plot below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span> <span class="n">data</span> <span class="o">=</span> <span class="n">df</span> <span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;sun&#39;</span> <span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;moon&#39;</span> <span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">20</span> <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">35</span><span class="p">)</span>

<span class="k">for</span> <span class="n">index</span><span class="p">,</span><span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;sun&#39;</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;moon&#39;</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span> <span class="p">)</span>

    
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">17</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">],</span> <span class="s1">&#39;r:&#39;</span> <span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">26</span><span class="p">],</span> <span class="s1">&#39;r:&#39;</span> <span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span> <span class="mf">5.5</span> <span class="p">,</span> <span class="mi">9</span> <span class="p">,</span> <span class="s1">&#39;cos(θ)&#39;</span> <span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/11 Differences_and_Similarities_5_0.png" src="../_images/11 Differences_and_Similarities_5_0.png" />
</div>
</div>
<p>The cosine similarity is the <em>cosinus</em> of the angle in between these two lines. The calcluation of the cosine similarity results in a number in between 0 and 1. If the similarity is 1 (i.e. the cosinus of 0), this means that the two points are on the same line. Note that, while the euclidean distance gives an indication of the differences between two sets of numbers, the cosine similarity measures the degree to which these numbers are the same.</p>
<p>To convert the cosine similarity into a metric that indicates the distance, we simply need to subtract the cosine similarity from the number 1:</p>
<p>\( Dc(A,B) = 1 - Sc(A,B) \)</p>
<p>This measure is called the <em>cosine distance</em>.</p>
<p>The cosine distance largely ignores the dimensions of the data. The formula mainly considers the question whether the different points are on the same straight line. The <em>euclidean distance</em> between the points (1,2) and (100,200) would be quite large, but their cosine distance would be 0.</p>
<p>Like the Euclidean distance, it is also possible to calculate the cosine distance for data sets containing more than two variables. In the case of such multivariate data sets, we need to create the angles in multi-dimensional vectors.</p>
</div>
<div class="section" id="scikit-learn">
<h2>Scikit-learn<a class="headerlink" href="#scikit-learn" title="Permalink to this headline">¶</a></h2>
<p>In Python, you can calculate the Euclidean distance and the cosine distance using <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, a library that can be used for Machine Learning. This <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> library is shipped with the free Anaconda distribution of Python. The <code class="docutils literal notranslate"><span class="pre">euclidean_distances()</span></code> method from this library demands a Pandas data frame as a parameter. It returns a matrix which contains the euclidean distances between all the rows in the data frame. This shape of this matrix is a square: the number of rows and the number of columns are exacly the same. The number of rows and columns correspond to the number of rows in the data frame that was given as input to the <code class="docutils literal notranslate"><span class="pre">euclidean_distances()</span></code> method.</p>
<p>The code below using the method to calculate the euclidean distances between the two points in the <code class="docutils literal notranslate"><span class="pre">df</span></code> data frame that was discussed earlier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_distances</span> <span class="p">,</span> <span class="n">euclidean_distances</span>

<span class="n">matrix</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 0.        17.4642492]
 [17.4642492  0.       ]]
</pre></div>
</div>
</div>
</div>
<p>The data frame <code class="docutils literal notranslate"><span class="pre">df</span></code> contains two rows, and the matrix that is created by <code class="docutils literal notranslate"><span class="pre">euclidean_distances</span></code> consequently has two rows and two columns. The second value on the first row contains the euclidean distance between ‘Text1’ and ‘Text2’.</p>
<p>The method <code class="docutils literal notranslate"><span class="pre">cosine_distances()</span></code> works in a very similar way:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">matrix</span> <span class="o">=</span> <span class="n">cosine_distances</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.         0.21735864]
 [0.21735864 0.        ]]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="a-term-document-matrix">
<h2>A term-document matrix<a class="headerlink" href="#a-term-document-matrix" title="Permalink to this headline">¶</a></h2>
<p>The data frame that was used in the examples above contains only two columns. As was mentioned, however, it is also possible to calculate euclidean and cosine distances for date frames containing more than two columns.</p>
<p>Studies of the differences and the similarities between texts may be based on the frequencies of the words used in these texts. Such comparisons can be performed effectively on the basis of a so-called term-document matrix. This is data structure which captures word frequencies for all the texts in a corpus. In this data structure, the words that are counted form the columns. Each row captures the frequencies of these words in a specific text. The data frame named <code class="docutils literal notranslate"><span class="pre">df</span></code>, which was discussed earlier in this notebook, was in fact a very simple example of a term-document matrix. It described two texts and two words only, but the number of rows and the number of columns can clearly be expanded.</p>
<p>The code below can be used to create a term-document matrix for all the texts in your corpus. The code firstly identifies the 1000 most frequent words in the full corpus. Next, it calculates all the frequencies of these words within individual texts.</p>
<p>The results are saved in a data frame named <code class="docutils literal notranslate"><span class="pre">termdocmatrix</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tdm</span>
<span class="kn">from</span> <span class="nn">tdm</span> <span class="kn">import</span> <span class="n">word_tokenise</span> <span class="p">,</span> <span class="n">sortedByValue</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">os.path</span> <span class="kn">import</span> <span class="n">join</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="nb">dir</span> <span class="o">=</span> <span class="s1">&#39;Corpus&#39;</span>
<span class="n">numberOfWords</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">freq</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">mfw</span> <span class="o">=</span> <span class="p">[]</span>


<span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span> <span class="sa">f</span><span class="s1">&#39;Finding the </span><span class="si">{</span><span class="n">numberOfWords</span><span class="si">}</span><span class="s1"> most frequent words in the corpus ...&#39;</span> <span class="p">)</span>

<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="nb">dir</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span> <span class="sa">r</span><span class="s1">&#39;\.txt$&#39;</span> <span class="p">,</span> <span class="n">file</span> <span class="p">):</span>
        <span class="n">fullText</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span> <span class="n">join</span><span class="p">(</span> <span class="nb">dir</span> <span class="p">,</span> <span class="n">file</span> <span class="p">)</span> <span class="p">,</span> <span class="n">encoding</span> <span class="o">=</span> <span class="s1">&#39;utf-8&#39;</span> <span class="p">,</span> <span class="n">errors</span> <span class="o">=</span> <span class="s1">&#39;ignore&#39;</span> <span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">word_tokenise</span><span class="p">(</span><span class="n">fullText</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span> <span class="ow">and</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span> <span class="s1">&#39;\w&#39;</span> <span class="p">,</span> <span class="n">w</span> <span class="p">):</span>
                <span class="n">freq</span><span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span> <span class="o">=</span> <span class="n">freq</span><span class="o">.</span><span class="n">get</span><span class="p">(</span> <span class="n">w</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="p">,</span> <span class="mi">0</span> <span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
                
<span class="k">def</span> <span class="nf">sortedByValue</span><span class="p">(</span> <span class="nb">dict</span> <span class="p">):</span>      
    <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span> <span class="nb">dict</span> <span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">x</span><span class="p">])</span> 

<span class="n">all_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">sortedByValue</span><span class="p">(</span><span class="n">freq</span><span class="p">))</span> <span class="p">)</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">all_words</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]:</span>
    <span class="n">mfw</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>


<span class="n">termdocmatrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span> <span class="n">columns</span><span class="o">=</span> <span class="n">mfw</span> <span class="p">)</span>
<span class="n">nr_tokens</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>  
        
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="nb">dir</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span> <span class="sa">r</span><span class="s1">&#39;\.txt$&#39;</span> <span class="p">,</span> <span class="n">file</span> <span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span> <span class="sa">f</span><span class="s1">&#39;Calculating the word frequencies for </span><span class="si">{</span><span class="n">file</span><span class="si">}</span><span class="s1"> ...&#39;</span> <span class="p">)</span>
        <span class="n">freq</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">new_row</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">fullText</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span> <span class="nb">dir</span> <span class="p">,</span> <span class="n">file</span> <span class="p">)</span> <span class="p">,</span> <span class="n">encoding</span> <span class="o">=</span> <span class="s1">&#39;utf-8&#39;</span> <span class="p">,</span> <span class="n">errors</span> <span class="o">=</span> <span class="s1">&#39;ignore&#39;</span> <span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">tdm</span><span class="o">.</span><span class="n">word_tokenise</span><span class="p">(</span><span class="n">fullText</span><span class="p">)</span>
        <span class="n">title</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span> <span class="sa">r</span><span class="s1">&#39;\.txt$&#39;</span> <span class="p">,</span> <span class="s1">&#39;&#39;</span> <span class="p">,</span> <span class="n">file</span><span class="p">)</span>
        <span class="n">new_row</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">title</span>
        <span class="n">nr_tokens</span><span class="p">[</span><span class="n">title</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
            <span class="n">freq</span><span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span> <span class="o">=</span> <span class="n">freq</span><span class="o">.</span><span class="n">get</span><span class="p">(</span> <span class="n">w</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="p">,</span> <span class="mi">0</span> <span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
  
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">mfw</span><span class="p">:</span>
            <span class="n">new_row</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">=</span> <span class="n">freq</span><span class="o">.</span><span class="n">get</span><span class="p">(</span> <span class="n">w</span> <span class="p">,</span> <span class="mi">0</span> <span class="p">)</span> 
            
        <span class="n">termdocmatrix</span> <span class="o">=</span> <span class="n">termdocmatrix</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">new_row</span> <span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">termdocmatrix</span> <span class="o">=</span><span class="n">termdocmatrix</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;index&#39;</span><span class="p">)</span>        
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Done!&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Once this large term-document matrix is created, we can create the cosine distances between all the texts using <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">matrix</span> <span class="o">=</span> <span class="n">cosine_distances</span><span class="p">(</span><span class="n">termdocmatrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">cosine_distances()</span></code> method returns an object of type <code class="docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code>. The following code converts this object to a regular Pandas data frame. With this format, it becomes easier to visualise the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">titles</span> <span class="o">=</span> <span class="p">[]</span>
<span class="nb">dir</span> <span class="o">=</span> <span class="s1">&#39;Corpus&#39;</span>

<span class="n">titles</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">termdocmatrix</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">matrix_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span> <span class="n">matrix</span> <span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">titles</span> <span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">titles</span> <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="a-heat-map">
<h2>A heat map<a class="headerlink" href="#a-heat-map" title="Permalink to this headline">¶</a></h2>
<p>The matrix containing the cosine distances can be visualised effectively as a heat map. Such a graph can be created using the <code class="docutils literal notranslate"><span class="pre">heatmap()</span></code> method in <code class="docutils literal notranslate"><span class="pre">Seaborn</span></code>.</p>
<p>A heat map is a diagram which consists of tiles in specific colours. Data values can be represented using the gradients in between two colours. A heat map often helps to identify specific notable values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="c1"># Heatmap</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span> <span class="n">matrix_df</span> <span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;YlGnBu&quot;</span>  <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/11 Differences_and_Similarities_18_0.png" src="../_images/11 Differences_and_Similarities_18_0.png" />
</div>
</div>
</div>
<div class="section" id="a-dendrogram">
<h2>A dendrogram<a class="headerlink" href="#a-dendrogram" title="Permalink to this headline">¶</a></h2>
<p>The results of these measures of distances can also be visualised using dendrograms. They are diagrams which divide corpora into clusters, based on an analysis of the overall differences between the texts. In such dendrograms, the texts which are most similar form a single branch, and texts which display fewer similarities do not form a union until a much later stage. An analysis based on dendrograms is also referred to as a <em>hierarchical cluster analysis</em>.</p>
<p>Dendrograms can be created using the code below. This code may generate a warning, but this warning may be ignored.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">linkage</span><span class="p">,</span> <span class="n">dendrogram</span>
<span class="n">linkages</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">matrix_df</span><span class="p">,</span><span class="s1">&#39;ward&#39;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">dendrogram</span><span class="p">(</span> <span class="n">linkages</span> <span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">matrix_df</span><span class="o">.</span><span class="n">index</span> <span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">,</span> <span class="n">leaf_font_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">leaf_rotation</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ipython-input-9-73a56d2dce66&gt;:2: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix
  linkages = linkage(matrix_df,&#39;ward&#39;)
</pre></div>
</div>
<img alt="../_images/11 Differences_and_Similarities_20_1.png" src="../_images/11 Differences_and_Similarities_20_1.png" />
</div>
</div>
</div>
<div class="section" id="a-normalised-term-document-matrix">
<h2>A normalised term-document matrix<a class="headerlink" href="#a-normalised-term-document-matrix" title="Permalink to this headline">¶</a></h2>
<p>The term-document matrix that was created earlier, and which was stored as a data frame named <code class="docutils literal notranslate"><span class="pre">termdocmatrix</span></code>, contains the absolute counts of the words. In other words, it contains the total number of times the given word occurs in each text. These absolute counts cannot easily be compared, as the texts are clearly of a different length. Their total token counts are different.</p>
<p>For the calculation of the cosine distance, these different lengths don’t have much of an impact. As was explained, this measure of difference mostly considers proportions and largely overlooks the actual dimensions. The cosine distance between points (10,20) and (100,200) is zero, for instance. The cosine distance formula can be used effectively, for this reason, with texts with different lengths.</p>
<p>These dimmension do make a difference, however, in the calculation of the Euclidean distance. In order to work with this specific measure of difference, it is useful to create a normalised term-document matrix. This is a matrix in which all the absolute frequencies are divided by the total number of tokens in each text. After this division, the numbers will represent percentages or ratios. These ratios allow for a fairer comparison.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Create a new blank data frame</span>
<span class="n">termdocmatrix_norm</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span> <span class="n">columns</span><span class="o">=</span> <span class="n">mfw</span> <span class="p">)</span>

<span class="c1">## The columns in &#39;termdocmatrix&#39; are the </span>
<span class="c1">## 1000 most frequent word</span>
<span class="n">mfw</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span> <span class="n">termdocmatrix</span><span class="o">.</span><span class="n">columns</span> <span class="p">)</span>

<span class="c1">## the new blank data frame named termdocmatrix_norm</span>
<span class="c1"># is filled: In each column, the absolue counts</span>
<span class="c1"># are divided by the number of tokens </span>
<span class="c1"># the dictionary &#39;nr_tokens&#39; was created in an earlier cell</span>
<span class="k">for</span> <span class="n">text</span><span class="p">,</span><span class="n">word</span> <span class="ow">in</span> <span class="n">termdocmatrix</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">new_row</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="n">new_row</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">mfw</span><span class="p">:</span>
        <span class="n">new_row</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">=</span> <span class="n">word</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">/</span> <span class="n">nr_tokens</span><span class="p">[</span><span class="n">text</span><span class="p">]</span>
    <span class="n">termdocmatrix_norm</span> <span class="o">=</span> <span class="n">termdocmatrix_norm</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">new_row</span> <span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">termdocmatrix_norm</span> <span class="o">=</span> <span class="n">termdocmatrix_norm</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;index&#39;</span><span class="p">)</span>        
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Done!&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The normalised term-document matrix, named <code class="docutils literal notranslate"><span class="pre">termdocmatrix_norm</span></code>, can be used with the method <code class="docutils literal notranslate"><span class="pre">euclidean_distances()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">matrix</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">termdocmatrix_norm</span><span class="p">)</span>

<span class="n">titles</span> <span class="o">=</span> <span class="p">[]</span>
<span class="nb">dir</span> <span class="o">=</span> <span class="s1">&#39;Corpus&#39;</span>

<span class="n">titles</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">termdocmatrix_norm</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
        
<span class="n">matrix_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span> <span class="n">matrix</span> <span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">titles</span> <span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">titles</span> <span class="p">)</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="c1"># Heatmap</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span> <span class="n">matrix_df</span> <span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;YlGnBu&quot;</span>  <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/11 Differences_and_Similarities_25_0.png" src="../_images/11 Differences_and_Similarities_25_0.png" />
</div>
</div>
</div>
<div class="section" id="a-network">
<h2>A network<a class="headerlink" href="#a-network" title="Permalink to this headline">¶</a></h2>
<p>The analyses that have been discussed above were all based on measures of differences. As was mentioned, however, the cosine similarity does not focus on difference, but on the degree to which texts are identical.</p>
<p>The cosine similarity may be used to develop a network representation of a text corpus. A network, more broadly consists of a collection of nodes. These nodes can be connected by edges.</p>
<p>In such a network visualisation, we can represent all the texts in the coprus as nodes. Using the cosine similarity matrix, we decide whether the nodes should be connected via edges. In the code below, edges are added between two texts if their cosine similarity is 0.7 or higher.</p>
<p>The network is created and drawn using the <code class="docutils literal notranslate"><span class="pre">networkx</span></code> library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>
<span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>

<span class="n">matrix</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">termdocmatrix</span><span class="p">)</span>
<span class="n">matrix_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span> <span class="n">matrix</span> <span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">titles</span> <span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">titles</span> <span class="p">)</span>
            
                
<span class="n">nodes</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="n">edges</span> <span class="o">=</span> <span class="p">[]</span>


<span class="n">related_texts</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">matrix_df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="c1">## an edge is drawn in between two nodes</span>
<span class="c1"># if the cosine similarity is 0.7 or higher</span>
<span class="n">min_similarity</span> <span class="o">=</span> <span class="mf">0.7</span>

<span class="k">for</span> <span class="n">text</span><span class="p">,</span><span class="n">values</span> <span class="ow">in</span> <span class="n">matrix_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">rt</span> <span class="ow">in</span> <span class="n">related_texts</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">text</span> <span class="o">!=</span> <span class="n">rt</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">values</span><span class="p">[</span><span class="n">rt</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">min_similarity</span><span class="p">:</span>
                <span class="n">edges</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="p">(</span><span class="n">text</span><span class="p">,</span><span class="n">rt</span><span class="p">)</span> <span class="p">)</span>
                <span class="n">nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="n">nodes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span> <span class="nb">set</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span> <span class="p">)</span>

<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_nodes_from</span><span class="p">(</span> <span class="n">nodes</span> <span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edges_from</span><span class="p">(</span> <span class="n">edges</span> <span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">G</span> <span class="p">,</span> <span class="n">node_size</span><span class="o">=</span><span class="mi">150</span> <span class="p">,</span> <span class="n">with_labels</span> <span class="o">=</span> <span class="kc">True</span> <span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s1">&#39;#6f12a1&#39;</span><span class="p">)</span>
                
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/11 Differences_and_Similarities_28_0.png" src="../_images/11 Differences_and_Similarities_28_0.png" />
</div>
</div>
</div>
<div class="section" id="principal-component-analysis">
<h2>Principal component analysis<a class="headerlink" href="#principal-component-analysis" title="Permalink to this headline">¶</a></h2>
<p>Differences and similarities between texts can also be compared via a Principal Component Analysis (PCA). Simply put, this is a form of multi-variate analysis in which a large number of variables can be replaced by a much smaller number of variables. The method aims to create new variables which can account for most of the variability in the full data set. These new variables are referred to as the principal components. If the first two principal components account for most of the variability, the global distribution of the values in your data can be clarified by plotting these two principal components on a scatter plot.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Scikit-learn</span></code> library, which was mentioned above, contains many methods in the field of machine learning. It also contains a number of methods for working with PCA.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">PCA()</span></code> method, form the <code class="docutils literal notranslate"><span class="pre">sklearn.decomposition</span></code> module, can be used to create an object which can carry out to calculation. While creating this object, you need to specify the number of components that you want to work with. This new object that results from <code class="docutils literal notranslate"><span class="pre">PCA()</span></code> has a method named fit_transform(). When you supply a data frame containing numbers as its parameter, the output will be a set of principal components. To make the visualisation easier, the <code class="docutils literal notranslate"><span class="pre">principalComponents</span></code> object is converted into a Pandas data frame.</p>
<p>The first two principal component can then be visualised using a scatter plot. The texts that are close to each other in the scatter plot are also assumed to use roughly the same words, in roughly the same frequencies.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">re</span>


<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;darkgrid&quot;</span><span class="p">)</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">principalComponents</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">termdocmatrix_norm</span><span class="p">)</span>
<span class="n">pcDf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">principalComponents</span> <span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;pc1&#39;</span><span class="p">,</span> <span class="s1">&#39;pc2&#39;</span><span class="p">])</span>


<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span> <span class="s1">&#39;metadata.csv&#39;</span> <span class="p">)</span>
<span class="n">pcDf</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span>
<span class="c1">#pcDf[&#39;class&#39;] = df[&#39;class&#39;]</span>


<span class="c1">## plot principal component in a scatter plot</span>


<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">mpatches</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span> <span class="n">data</span> <span class="o">=</span> <span class="n">pcDf</span> <span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;pc1&#39;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;pc2&#39;</span> <span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">70</span>  <span class="p">)</span>


<span class="n">xlim</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.0075</span><span class="p">,</span><span class="mf">0.005</span> <span class="p">)</span>
<span class="n">ylim</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.002</span><span class="p">,</span><span class="mf">0.0015</span> <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span> <span class="n">xlim</span> <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span> <span class="n">ylim</span> <span class="p">)</span>

<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">pcDf</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;pc1&#39;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;pc2&#39;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;pc1&#39;</span><span class="p">]</span>  <span class="p">,</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;pc2&#39;</span><span class="p">]</span>  <span class="p">,</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span> <span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span> 


<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/11 Differences_and_Similarities_30_0.png" src="../_images/11 Differences_and_Similarities_30_0.png" />
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "cdsleiden/tdm-tutorial",
            ref: "gh-pages",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="10%20Named_Entity_Recognition.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">10. Named Entity Recognition</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="12%20Topic%20modelling.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">12. Topic modelling</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Peter Verhaar<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>